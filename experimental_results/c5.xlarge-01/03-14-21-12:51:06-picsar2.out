--------------------------------------------------------------------------
[[27559,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: ip-172-31-2-78

Another transport will be used instead, although this may result in
lower performance.

NOTE: You can disable this warning by setting the MCA parameter
btl_base_warn_component_unused to 0.
--------------------------------------------------------------------------
 _________________________________________________________________
 
  PICSAR
 _________________________________________________________________
 Cartesian topology
 Processor subdivision is                     1                    1                    1

 
 SIMULATION PARAMETERS:
 Dimension:                    2
 dx, dy, dz:   1.0050251256281407E-007   1.7976931348623157E+308   1.0050251256281407E-007
 dt:   3.3524029668155986E-016 s  0.33524029232045371      fs
 Coefficient on dt determined via the CFL (dtcoef):      0.70000
 Total time:   6.3352744810042019      plasma periods:   3.3859269964837547E-014 s
 Number of steps:                  101
 Tiles:                    1                    1                    1
 MPI com current:                    0
 Current deposition method:                    0
 Charge deposition algo:                    0
 Field gathering method:                    0
 Field gathering plus particle pusher seperated:                    0
 Current/field gathering order:                    2                    2                    2
 Particle communication: (partcom=1)
 Pusher: Boris algorithm (particle_pusher=0)
 MPI buffer size:                 2000
 
 Vector length current deposition                    8
 Vector length charge deposition                   64
 Vector length field gathering                  256
 
 PLASMA PROPERTIES:
 Distribution:                    1
 Density in the lab frame:   1.1000000000000000E+025 m^-3
 Density in the simulation frame:   1.1000000000000000E+025 m^-3
 Cold plasma frequency in the lab frame:   187106056556544.47      s^-1
 cold plasma wavelength:   1.0067293394853861E-005 m   10.067293394853861      um
 
 MPI domain decomposition
 Topology:                    0
 Local number of cells:  199     1   199
 Local number of grid point:  200     2   200
 Guard cells:    3     0     3
 
 Periodic field bcs X axis
 Periodic field bcs Y axis
 Periodic field bcs Z axis
 Particle sorting activated
 dx:   5.0251256281407036E-008
 dy:   8.9884656743115785E+307
 dz:   5.0251256281407036E-008
 shiftx:  -5.0251256281407036E-008
 shifty:  -8.9884656743115785E+307
 shiftz:  -5.0251256281407036E-008
 
 Number of species:                    2
 laser_antenna
 Charge:   1.0000000000000000     
 Drift velocity:   0.0000000000000000        0.0000000000000000        0.0000000000000000     
 Sorting period:                    0
 Sorting start:                    0
 
 laser_antenna
 Charge:  -1.0000000000000000     
 Drift velocity:   0.0000000000000000        0.0000000000000000        0.0000000000000000     
 Sorting period:                    0
 Sorting start:                    0
 
 Output of time statistics non-activated
 Computation of the time statistics starts at iteration:    0

 No particle dump ( 0)

 Set tile split: done
 Initialization of the tile arrays: done
 number of cells per laser wavelength   28.142849891224589     
 Laser Waist   2000000.0000000000      m
 Laser duration   1.0006922855944562E-014  (in s)
 Laser duration   29.850000000000001       (in dt)
 Laser peak    0.0000000000000000      dt
 Laser longitudinal length   3.0000000000000001E-006 m
 Laser temporal frequency w_laser   470912891827213.38      s^-1
 Laser temporal period   1.3342563807926080E-014 s
 Laser temporal period   39.799999999999997      dt
 Laser tau / laser period   0.75000000000000011     
 number of cells per laser wavelength   28.142849891224589     
 LASER EMAX   802675290077.20142        802675290077.20142        0.0000000000000000     
 Creation of the particles: done
 Total memory (GB) for grid arrays    9.5056640000000005E-003
 Total memory (GB) for grid tile arrays    2.0369279999999999E-003
 Total memory (GB) for particle tile arrays    1.0137856000000001E-002
 Total memory (GB)   2.1680448000000001E-002
 Avg memory (GB) /MPI process    2.1680448000000001E-002

  Initilization of the diags
mkdir: cannot create directory ‘RESULTS’: File exists

 nsteps =                   101
[PI-INFO] Init time,0,0.126848
 it =                     1  || time =    3.3524029668155986E-016  || push/part (ns)=                   Infinity  || tot/part (ns)=                   Infinity
 Fields dump in    2.3400002646667417E-007  (s)
[PI-INFO] Paramount Iteration,0,1,0.139427,0.012580
 it =                     2  || time =    6.7048059336311973E-016  || push/part (ns)=                   Infinity  || tot/part (ns)=                   Infinity
[PI-INFO] Paramount Iteration,0,2,0.148647,0.009220
 it =                     3  || time =    1.0057208900446796E-015  || push/part (ns)=                   Infinity  || tot/part (ns)=                   Infinity
[PI-INFO] Paramount Iteration,0,3,0.158785,0.010138
 it =                     4  || time =    1.3409611867262395E-015  || push/part (ns)=                   Infinity  || tot/part (ns)=                   Infinity
[PI-INFO] Paramount Iteration,0,4,0.168338,0.009553
 it =                     5  || time =    1.6762014834077993E-015  || push/part (ns)=                   Infinity  || tot/part (ns)=                   Infinity
[PI-INFO] Paramount Iteration,0,5,0.177664,0.009326
 it =                     6  || time =    2.0114417800893593E-015  || push/part (ns)=                   Infinity  || tot/part (ns)=                   Infinity
[PI-INFO] Paramount Iteration,0,6,0.187005,0.009341
 it =                     7  || time =    2.3466820767709191E-015  || push/part (ns)=                   Infinity  || tot/part (ns)=                   Infinity
[PI-INFO] Paramount Iteration,0,7,0.196964,0.009959
 it =                     8  || time =    2.6819223734524789E-015  || push/part (ns)=                   Infinity  || tot/part (ns)=                   Infinity
[PI-INFO] Paramount Iteration,0,8,0.207247,0.010283
 it =                     9  || time =    3.0171626701340387E-015  || push/part (ns)=                   Infinity  || tot/part (ns)=                   Infinity
[PI-INFO] Paramount Iteration,0,9,0.217198,0.009951
 it =                    10  || time =    3.3524029668155985E-015  || push/part (ns)=                   Infinity  || tot/part (ns)=                   Infinity
[PI-INFO] Paramount Iteration,0,10,0.226684,0.009486
 it =                    11  || time =    3.6876432634971587E-015  || push/part (ns)=                   Infinity  || tot/part (ns)=                   Infinity
[PI-INFO] Paramount Iteration,0,11,0.236628,0.009944
 it =                    12  || time =    4.0228835601787186E-015  || push/part (ns)=                   Infinity  || tot/part (ns)=                   Infinity
[PI-INFO] Paramount Iteration,0,12,0.247188,0.010560
 it =                    13  || time =    4.3581238568602784E-015  || push/part (ns)=                   Infinity  || tot/part (ns)=                   Infinity
[PI-INFO] Paramount Iteration,0,13,0.257582,0.010394
 it =                    14  || time =    4.6933641535418382E-015  || push/part (ns)=                   Infinity  || tot/part (ns)=                   Infinity
[PI-INFO] Paramount Iteration,0,14,0.268033,0.010451
 it =                    15  || time =    5.0286044502233980E-015  || push/part (ns)=                   Infinity  || tot/part (ns)=                   Infinity
[PI-INFO] Paramount Iteration,0,15,0.278287,0.010254
[PI-INFO] PI avg,0,0.010096,15
[PI-INFO] Beta,0,0.000087
[PI-INFO] Total time,0.278300
--------------------------------------------------------------------------
mpirun has exited due to process rank 0 with PID 0 on
node ip-172-31-2-78 exiting improperly. There are three reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpirun (as reported here).

You can avoid this message by specifying -quiet on the mpirun command line.
--------------------------------------------------------------------------
Command exited with non-zero status 1
	Command being timed: "mpirun --hostfile /home/ubuntu/PI-Bench/ECP-Proxy-Apps/hostfile -n 1 ./fortran_bin/picsar examples/example_decks_fortran/plane_wave_test_2d-1.pixr"
	User time (seconds): 0.13
	System time (seconds): 0.08
	Percent of CPU this job got: 9%
	Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.33
	Average shared text size (kbytes): 0
	Average unshared data size (kbytes): 0
	Average stack size (kbytes): 0
	Average total size (kbytes): 0
	Maximum resident set size (kbytes): 35144
	Average resident set size (kbytes): 0
	Major (requiring I/O) page faults: 4
	Minor (reclaiming a frame) page faults: 42614
	Voluntary context switches: 811
	Involuntary context switches: 9
	Swaps: 0
	File system inputs: 0
	File system outputs: 16432
	Socket messages sent: 0
	Socket messages received: 0
	Signals delivered: 0
	Page size (bytes): 4096
	Exit status: 1
